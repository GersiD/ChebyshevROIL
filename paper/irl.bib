@article{MEHTA2007237,
  title = "Optimal detection and control strategies for invasive species
           management",
  journal = "Ecological Economics",
  volume = "61",
  number = "2",
  pages = "237 - 245",
  year = "2007",
  issn = "0921-8009",
  doi = "https://doi.org/10.1016/j.ecolecon.2006.10.024",
  url = "http://www.sciencedirect.com/science/article/pii/S0921800906005568",
  author = "Shefali V. Mehta and Robert G. Haight and Frances R. Homans and
            Stephen Polasky and Robert C. Venette",
  keywords = "Invasive species, Non-native, Detection, Risk management",
}

@article{ecs2.1263,
  author = {Meisner, Matthew H. and Rosenheim, Jay A. and Tagkopoulos, Ilias},
  title = {A data-driven, machine learning framework for optimal pest management
           in cotton},
  journal = {Ecosphere},
  year = "2016",
  volume = {7},
  number = {3},
  pages = {e01263},
  keywords = {agricultural informatics, applied pest management, Markov decision
              processes, optimal pest management},
  doi = {10.1002/ecs2.1263},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecs2.1263},
  eprint = {
            https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecs2.1263
            },
  abstract = {Abstract Despite the significant effects of agricultural pest
              management on crop yield, profit, environmental quality, and
              sustainability, farmers oftentimes lack data-driven decision
              support to help optimize pest management strategies. To address
              this need, we curated a comprehensive data set that consists of
              pest, pest management, and yield information from 1498 commercial
              cotton crops in California's San Joaquin Valley between 1997 and
              2008. Using this data set, we built a Markov decision process model
              to identify the optimal management policy of a key cotton pest,
              Lygus hesperus, that balances the tradeoff between yield loss and
              the cost of pesticide applications. Our results show that pesticide
              applications targeting L. hesperus are only economically optimal
              during the first 2 weeks of June, and pesticide applications were
              associated with increased risk of an unprofitable harvest. About 46
              \% of the observations in our data set involved at least one
              pesticide application outside of this optimal window, demonstrating
              the need for a data-driven approach to crop management. Sensitivity
              analyses on parameter perturbations and reduced data set sizes
              suggest that our methodology provides a robust policy-making tool,
              even in noisy data sets.},
}

@book{boyd_convex_optimization,
  author = {Boyd and Vandenberghe},
  title = {Convex Optimization},
  publisher = "Cambridge University Press",
  year = {2004},
}

@article{taleghan15a,
  author = {Majid Alkaee Taleghan and Thomas G. Dietterich and Mark Crowley and
            Kim Hall and H. Jo Albers},
  title = {PAC Optimal MDP Planning with Application to Invasive Species
           Management},
  journal = {Journal of Machine Learning Research},
  year = {2015},
  volume = {16},
  pages = {3877-3903},
  url = {http://jmlr.org/papers/v16/taleghan15a.html},
}

@article{Hall2018,
  author = "Hall, Kim Meyer and Albers, Heidi J. and Alkaee Taleghan, Majid and
            Dietterich, Thomas G.",
  title = "Optimal Spatial-Dynamic Management of Stochastic Species Invasions",
  journal = "Environmental and Resource Economics",
  year = "2018",
  month = "Jun",
  day = "01",
  volume = "70",
  number = "2",
  pages = "403--427",
  abstract = "Recent analyses demonstrate that the spatial--temporal behavior of
              invasive species requires optimal management decisions over space
              and time. From a spatial perspective, this bioeconomic optimization
              model broadens away from invasive species spread at a frontier or
              to neighbors by examining short and long-distance dispersal,
              directionality in spread, and network geometry. In terms of
              uncertainty and dynamics, this framework incorporates several
              sources of stochasticity, decisions with multi-year implications,
              and temporal ecological processes. This paper employs a unique
              Markov decision process planning algorithm and a Monte Carlo
              simulation of the stochastic system to explore the spatial-dynamic
              optimal policy for a river network facing a bioinvasion, with
              Tamarisk as an example. In addition to exploring the spatial,
              stochastic, and dynamic aspects of management of invasions, the
              results demonstrate how the interaction of spatial and multi-period
              processes contributes to finding the optimal policy. Those
              interactions prove critical in determining the right management
              tool, in the right location, at the right time, which informs the
              management implications drawn from simpler frameworks. In
              particular, as compared to other modeling framework's policy
              prescriptions, the framework here finds more use of the management
              tool restoration and more management in highly connected locations,
              which leads to a less invaded system over time.",
  issn = "1573-1502",
  doi = "10.1007/s10640-017-0127-6",
  url = "https://doi.org/10.1007/s10640-017-0127-6",
}

@article{ALBERS201844,
  title = "The Role of Restoration and Key Ecological Invasion Mechanisms in
           Optimal Spatial-Dynamic Management of Invasive Species",
  journal = "Ecological Economics",
  volume = "151",
  pages = "44 - 54",
  year = "2018",
  issn = "0921-8009",
  doi = "https://doi.org/10.1016/j.ecolecon.2018.03.031",
  url = "http://www.sciencedirect.com/science/article/pii/S0921800917310455",
  author = "Heidi J. Albers and Kim Meyer Hall and Katherine D. Lee and Majid
            Alkaee Taleghan and Thomas G. Dietterich",
  keywords = "Spatial, Dynamic optimization, Meta-population, Dispersal, Species
              interactions, Species competition, Bio-invasions, Invasive species,
              Ecology, River network, Restoration, Habitat, Riparian",
}

@article{doi:10.1111,
  author = {Nicol, Sam and Sabbadin, Regis and Peyrard, Nathalie and Chadès,
            Iadine},
  title = {Finding the best management policy to eradicate invasive species from
           spatial ecological networks with simultaneous actions},
  journal = {Journal of Applied Ecology},
  volume = {54},
  number = {6},
  pages = {1989-1999},
  keywords = {classification trees, computational sustainability, conservation,
              Gambusia, invasive species, Markov decision processes, network,
              optimal control, optimisation, Scaturiginichthys vermeilipinnis},
  doi = {10.1111/1365-2664.12884},
  url = {
         https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1365-2664.12884
         },
  eprint = {
            https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2664.12884
            },
  abstract = {Summary Spatial management of invasive species is more likely to
              be successful when multiple locations are treated simultaneously.
              However, selecting the best locations to act is difficult due to
              the many options available at any time. We design a near-optimal
              policy for applying multiple actions simultaneously for faster
              invasive species control within a network. Our method uses a recent
              optimisation tool, the graph-based Markov decision process (GMDP).
              Since the policy can be difficult to interpret, we extracted a
              simpler policy using classification trees. We applied our approach
              to the eradication of invasive mosquitofish Gambusia holbrooki from
              the habitat of the red-finned blue-eye Scaturiginichthys
              vermeilipinnis, a critically endangered fish with a global
              population that is restricted to seven artesian springs in
              Queensland, Australia. The policy returned by the GMDP was to
              manage springs occupied by mosquitofish and their connected
              neighbours, unless the neighbours were occupied by red-finned
              blue-eyes. Simultaneous management resulted in rapid declines in
              simulated mosquitofish occupancy even if eradication effectiveness
              was low; however, the cost of simultaneous eradication was high and
              sustained eradication effort was necessary to maintain low
              mosquitofish occupancy. Synthesis and applications. Our paper finds
              a near-optimal, multi-action control policy to remove an invasive
              species from a multi-species spatial network. We introduce the
              graph-based Markov decision process and apply it to a real case
              study – eradication of invasive mosquitofish from the habitat of
              the red-finned blue-eye. We find that the graph-based Markov
              decision process can generate policies for networks with extremely
              large state spaces; however, it works best when nodes have fewer
              than five neighbours. We conclude that simultaneous eradications
              are effective for rapid control of invasive species; however,
              managers should consider the cost and time required for an
              effective eradication program.},
}

@article{DeFarias2003,
  abstract = {The curse of dimensionality gives rise to prohibitive
              computational requirements that render infeasible the exact
              solution of large-scale stochastic control problems. We study an
              efficient method based on linear programming for approximating
              solutions to such problems. The approach "fits" a linear
              combination of pre-selected basis functions to the dynamic
              programming cost-to-go function. We develop error bounds that offer
              performance guarantees and also guide the selection of both basis
              functions and "state-relevance weights" that influence quality of
              the approximation. Experimental results in the domain of queueing
              network control provide empirical support for the methodology.},
  author = {de Farias, D. P. and {Van Roy}, B.},
  doi = {10.1287/opre.51.6.850.24925},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/de Farias, Van Roy - 2003 - The Linear Programming
          Approach to Approximate Dynamic Programming.pdf:pdf},
  isbn = {0030-364X},
  issn = {0030-364X},
  journal = {Operations Research},
  mendeley-groups = {RL},
  number = {6},
  pages = {850--865},
  title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
  volume = {51},
  year = {2003},
}

@inproceedings{Regan2012,
  author = {Regan, Kevin and Boutilier, Craig},
  title = {Regret-Based Reward Elicitation for Markov Decision Processes},
  year = {2009},
  isbn = {9780974903958},
  publisher = {AUAI Press},
  address = {Arlington, Virginia, USA},
  abstract = {The specification of a Markov decision process (MDP) can be
              difficult. Reward function specification is especially problematic;
              in practice, it is often cognitively complex and time-consuming for
              users to precisely specify rewards. This work casts the problem of
              specifying rewards as one of preference elicitation and aims to
              minimize the degree of precision with which a reward function must
              be specified while still allowing optimal or near-optimal policies
              to be produced. We first discuss how robust policies can be
              computed for MDPs given only partial reward information using the
              minimax regret criterion. We then demonstrate how regret can be
              reduced by efficiently eliciting reward information using bound
              queries, using regret-reduction as a means for choosing suitable
              queries. Empirical results demonstrate that regret-based reward
              elicitation offers an effective way to produce near-optimal
              policies without resorting to the precise specification of the
              entire reward function.},
  booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in
               Artificial Intelligence},
  pages = {444–451},
  numpages = {8},
  location = {Montreal, Quebec, Canada},
  series = {UAI '09},
}

@article{Petrik2010a,
  abstract = {Approximate dynamic programming has been used successfully in a
              large variety of domains, but it relies on a small set of provided
              approximation features to calculate solutions reliably. Large and
              rich sets of features can cause existing algorithms to overfit
              because of a limited number of samples. We address this shortcoming
              using {\$}L{\_}1{\$} regularization in approximate linear
              programming. Because the proposed method can automatically select
              the appropriate richness of features, its performance does not
              degrade with an increasing number of features. These results rely
              on new and stronger sampling bounds for regularized approximate
              linear programs. We also propose a computationally efficient
              homotopy method. The empirical evaluation of the approach shows
              that the proposed method performs well on simple MDPs and standard
              benchmark problems.},
  archivePrefix = {arXiv},
  arxivId = {1005.1860},
  author = {Petrik, Marek and Taylor, Gavin and Parr, Ron and Zilberstein,
            Shlomo},
  eprint = {1005.1860},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/Petrik et al. - 2010 - Feature Selection Using
          Regularization in Approximate Linear Programs for Markov Decision
          Processes(2).pdf:pdf},
  title = {{Feature Selection Using Regularization in Approximate Linear
           Programs for Markov Decision Processes}},
  url = {http://arxiv.org/abs/1005.1860},
  year = {2010},
}


@article{VanRoy2002,
  author = {Daniela P. de Farias, Benjamin Van Roy},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/Van Roy - 2002 - Approximate Dynamic Programming via
          Linear Programming(2).pdf:pdf},
  journal = {Comparative and General Pharmacology},
  keywords = {kernel regression,linear programming,support vector machines},
  number = {1995},
  pages = {255--269},
  title = {{Approximate Dynamic Programming via Linear Programming}},
  year = {2002},
}

@article{VanRoy2006c,
  author = {{Van Roy}, Benjamin and Benjamin},
  doi = {10.1287/moor.1060.0188},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/Van Roy - 2006 - Performance Loss Bounds for
          Approximate Value Iteration with State Aggregation(2).pdf:pdf},
  issn = {0364-765X},
  journal = {Mathematics of Operations Research},
  keywords = {approximate value iteration,state aggregation,temporal-difference
              learning},
  mendeley-groups = {RL},
  month = {may},
  number = {2},
  pages = {234--244},
  publisher = {INFORMS},
  title = {{Performance Loss Bounds for Approximate Value Iteration with State
           Aggregation}},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.1060.0188},
  volume = {31},
  year = {2006},
}

@article{DeFarias2003,
  abstract = {The curse of dimensionality gives rise to prohibitive
              computational requirements that render infeasible the exact
              solution of large-scale stochastic control problems. We study an
              efficient method based on linear programming for approximating
              solutions to such problems. The approach "fits" a linear
              combination of pre-selected basis functions to the dynamic
              programming cost-to-go function. We develop error bounds that offer
              performance guarantees and also guide the selection of both basis
              functions and "state-relevance weights" that influence quality of
              the approximation. Experimental results in the domain of queueing
              network control provide empirical support for the methodology.},
  author = {de Farias, D. P. and {Van Roy}, B.},
  doi = {10.1287/opre.51.6.850.24925},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/de Farias, Van Roy - 2003 - The Linear Programming
          Approach to Approximate Dynamic Programming.pdf:pdf},
  isbn = {0030-364X},
  issn = {0030-364X},
  journal = {Operations Research},
  mendeley-groups = {RL},
  number = {6},
  pages = {850--865},
  title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
  volume = {51},
  year = {2003},
}
@inproceedings{Huang2018,
  author = {Huang, Jessie and Wu, Fa and Precup, Doina and Cai, Yang},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N.
            Cesa-Bianchi and R. Garnett},
  pages = {9105--9114},
  publisher = {Curran Associates, Inc.},
  title = {Learning Safe Policies with Expert Guidance},
  url = {
         https://proceedings.neurips.cc/paper/2018/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf
         },
  volume = {31},
  year = {2018},
}

@inproceedings{abbeel2004,
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  year = {2004},
  isbn = {1581138385},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1015330.1015430},
  doi = {10.1145/1015330.1015430},
  abstract = {We consider learning in a Markov decision process where we are not
              explicitly given a reward function, but where instead we can
              observe an expert demonstrating the task that we want to learn to
              perform. This setting is useful in applications (such as the task
              of driving) where it may be difficult to write down an explicit
              reward function specifying exactly how different desiderata should
              be traded off. We think of the expert as trying to maximize a
              reward function that is expressible as a linear combination of
              known features, and give an algorithm for learning the task
              demonstrated by the expert. Our algorithm is based on using "
              inverse reinforcement learning" to try to recover the unknown
              reward function. We show that our algorithm terminates in a small
              number of iterations, and that even though we may never recover the
              expert's reward function, the policy output by the algorithm will
              attain performance close to that of the expert, where here
              performance is measured with respect to the expert's unknown reward
              function.},
  booktitle = {Proceedings of the Twenty-First International Conference on
               Machine Learning},
  pages = {1},
  numpages = {8},
  location = {Banff, Alberta, Canada},
  series = {ICML '04},
}

@book{Puterman1994,
  author = {Puterman, Martin L.},
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  year = {1994},
  isbn = {0471619779},
  publisher = {John Wiley; Sons, Inc.},
  address = {USA},
  edition = {1st},
  abstract = {From the Publisher:The past decade has seen considerable
              theoretical and applied research on Markov decision processes, as
              well as the growing use of these models in ecology, economics,
              communications engineering, and other fields where outcomes are
              uncertain and sequential decision-making processes are needed. A
              timely response to this increased activity, Martin L. Puterman's
              new work provides a uniquely up-to-date, unified, and rigorous
              treatment of the theoretical, computational, and applied research
              on Markov decision process models. It discusses all major research
              directions in the field, highlights many significant applications
              of Markov decision processes models, and explores numerous
              important topics that have previously been neglected or given
              cursory coverage in the literature. Markov Decision Processes
              focuses primarily on infinite horizon discrete time models and
              models with discrete time spaces while also examining models with
              arbitrary state spaces, finite horizon models, and continuous-time
              discrete state models. The book is organized around optimality
              criteria, using a common framework centered on the optimality
              (Bellman) equation for presenting results. The results are
              presented in a "theorem-proof" format and elaborated on through
              both discussion and examples, including results that are not
              available in any other book. A two-state Markov decision process
              model, presented in Chapter 3, is analyzed repeatedly throughout
              the book and demonstrates many results and algorithms. Markov
              Decision Processes covers recent research advances in such areas as
              countable state space models with average reward criterion,
              constrained models, and models with risk sensitive optimality
              criteria. It also explores several topics that have received little
              or no attention in other books, including modified policy iteration
              , multichain models with average reward criterion, and sensitive
              optimality. In addition, a Bibliographic Remarks section in each
              chapter comments on relevant historic},
}

@inproceedings{alg_for_irl_Ng,
  author = {Ng, Andrew Y. and Russell, Stuart J.},
  title = {Algorithms for Inverse Reinforcement Learning},
  year = {2000},
  isbn = {1558607072},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  booktitle = {Proceedings of the Seventeenth International Conference on
               Machine Learning},
  pages = {663–670},
  numpages = {8},
  series = {ICML '00},
}

@article{Chow2015,
  archivePrefix = {arXiv},
  arxivId = {1506.02188},
  author = {Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  eprint = {1506.02188},
  file = {:home/soheil/Dropbox/unh/re{\_}
          learning/documents/Chow.Tamar.Mannor.Pavone.NIPS15.pdf:pdf},
  keywords = {reward elicitation,risk,robust mdp},
  mendeley-tags = {reward elicitation,robust mdp,risk},
  month = {jun},
  pages = {1--21},
  title = {{Risk-Sensitive and Robust Decision-Making: a CVaR Optimization
           Approach}},
  url = {http://arxiv.org/abs/1506.02188},
  year = {2015},
}


@article{DeepakRamachandran2007,
  abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning
              the reward function underlying a Markov Decision Process given the
              dynamics of the system and the behaviour of an expert. IRL is
              motivated by situations where knowledge of the rewards is a goal by
              itself (as in preference elicitation) and by the task of
              apprenticeship learning (learning policies from an expert). In this
              paper we show how to combine prior knowledge and evidence from the
              expert's actions to derive a probability distribution over the
              space of reward functions. We present efficient algorithms that
              find solutions for the reward learning and apprenticeship learning
              tasks that generalize well over these distributions. Experimental
              results show strong improvement for our methods over previous
              heuristic-based approaches.},
  archivePrefix = {arXiv},
  arxivId = {1206.5264},
  author = {{Deepak Ramachandran} and {Eyal Amir}},
  doi = {10.1109/72.788640},
  eprint = {1206.5264},
  file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/IJCAI07-416.pdf:pdf},
  isbn = {978-1-4673-2755-8},
  issn = {10450823},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  keywords = {Markov-decision processes,action understanding,bayesian,inference,
              markov decision processes,reinforcement learning,theory mind},
  number = {5},
  pages = {2586--2591},
  pmid = {19574462},
  title = {{Bayesian Inverse Reinforcement Learning}},
  url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}
         q=intitle:Goal+Inference+as+Inverse+Planning{\#}0},
  volume = {10},
  year = {2007},
}

@article{Ng2000,
  author = {Ng, Andrew y. and {Stuart Russell}},
  file = {:home/soheil/.local/share/data/Mendeley Ltd./Mendeley
          Desktop/Downloaded/Ng, Stuart Russell - Unknown - algorithms for
          inverse reinforcement learning.pdf:pdf},
  journal = {in Proc. 17th International Conf. on Machine Learning},
  pages = {663----670},
  title = {{Algorithms for Inverse Reinforcement Learning}},
  url = {https://ai.stanford.edu/{~}ang/papers/icml00-irl.pdf},
  year = {2000},
}

@inproceedings{Syed2008,
  author = {Syed, Umar and Bowling, Michael and Schapire, Robert E.},
  title = {Apprenticeship Learning Using Linear Programming},
  year = {2008},
  isbn = {9781605582054},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1390156.1390286},
  doi = {10.1145/1390156.1390286},
  abstract = {In apprenticeship learning, the goal is to learn a policy in a
              Markov decision process that is at least as good as a policy
              demonstrated by an expert. The difficulty arises in that the MDP's
              true reward function is assumed to be unknown. We show how to frame
              apprenticeship learning as a linear programming problem, and show
              that using an off-the-shelf LP solver to solve this problem results
              in a substantial improvement in running time over existing
              methods---up to two orders of magnitude faster in our experiments.
              Additionally, our approach produces stationary policies, while all
              existing methods for apprenticeship learning output policies that
              are "mixed", i.e. randomized combinations of stationary policies.
              The technique used is general enough to convert any mixed policy to
              a stationary policy.},
  booktitle = {Proceedings of the 25th International Conference on Machine
               Learning},
  pages = {1032–1039},
  numpages = {8},
  location = {Helsinki, Finland},
  series = {ICML '08},
}

@article{Babe-Vroman2011,
  abstract = {In this paper, we apply tools from inverse reinforcement learning
              (IRL) to the problem of learning from (unlabeled) demonstration
              trajectories of behavior generated by varying "intentions" or
              objectives. We derive an EM approach that clusters observed
              trajectories by inferring the objectives for each cluster using any
              of several possible IRL methods, and then uses the constructed
              clusters to quickly identify the intent of a trajectory. We show
              that a natural approach to IRL - a gradient ascent method that
              modifies reward parameters to maximize the likelihood of the
              observed trajectories - is successful at quickly identifying
              unknown reward functions. We demonstrate these ideas in the context
              of apprenticeship learning by acquiring the preferences of a human
              driver in a simple highway car simulator. Copyright 2011 by the
              author(s)/owner(s).},
  author = {Babe-Vroman, Monica and Marivate, Vukosi and Subramanian, Kaushik
            and Littman, Michael},
  file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/478{\_}
          icmlpaper.pdf:pdf},
  journal = {Proceedings of the 28th International Conference on Machine
             Learning (ICML-11)},
  pages = {897--904},
  title = {{Apprenticeship Learning About Multiple Intentions}},
  url = {http://www.icml-2011.org/papers/478{\_}icmlpaper.pdf},
  year = {2011},
}

@article{howard1972,
  abstract = {This paper considers the maximization of certain equivalent reward
              generated by a Markov decision process with constant risk
              sensitivity. First, value iteration is used to optimize possibly
              time-varying processes of finite duration. Then a policy iteration
              procedure is developed to find the stationary policy with highest
              certain equivalent gain for the infinite duration case. A simple
              example demonstrates both procedures.},
  author = {Howard, Ronald A and Matheson, James E},
  doi = {10.1287/mnsc.18.7.356},
  file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/howard1972.pdf:pdf},
  journal = {Management Science},
  number = {7},
  pages = {356--369},
  title = {{Risk-Sensitive Markov Decision Processes}},
  url = {https://doi.org/10.1287/mnsc.18.7.356},
  volume = {18},
  year = {1972},
}

@article{filar1995,
  author = {J. A. {Filar} and D. {Krass} and K. W. {Ross}},
  journal = {IEEE Transactions on Automatic Control},
  title = {Percentile performance criteria for limiting average Markov decision
           processes},
  year = {1995},
  volume = {40},
  number = {1},
  pages = {2-10},
  keywords = {probability;Markov processes;linear programming;decision
              theory;percentile performance criteria;limiting average Markov
              decision processes;feasibility problem;infinite-horizon Markov
              decision processes;long-run limiting average reward;probability
              level;optimization problems;maximal achievable target
              levels;deterministic policy;multiple rewards;multiple
              constraints;LP-based formulation;Random variables;Probability
              distribution;Mathematics;Statistics;Optimal
              control;Testing;Operations research},
  doi = {10.1109/9.362904},
  ISSN = {0018-9286},
  month = {Jan},
}

@article{sobel_1982,
  title = {The variance of discounted Markov decision processes},
  volume = {19},
  DOI = {10.2307/3213832},
  number = {4},
  journal = {Journal of Applied Probability},
  publisher = {Cambridge University Press},
  author = {Sobel, Matthew J.},
  year = {1982},
  pages = {794–802},
}

@article{Bauerle2011,
  abstract = {We investigate the problem of minimizing the Average-Value-at-Risk
              (AVaR{\$}\tau{\$}) of the discounted cost over a finite and an
              infinite horizon which is generated by a Markov Decision Process
              (MDP). We show that this problem can be reduced to an ordinary MDP
              with extended state space and give conditions under which an
              optimal policy exists. We also give a time-consistent
              interpretation of the AVaR{\$}\tau{\$}. At the end we consider a
              numerical example which is a simple repeated casino game. It is
              used to discuss the influence of the risk aversion parameter {\$}
              \tau{\$} of the AVaR{\$}\tau{\$}-criterion.},
  author = {B{\"{a}}uerle, Nicole and Ott, Jonathan},
  doi = {10.1007/s00186-011-0367-0},
  issn = {1432-5217},
  journal = {Mathematical Methods of Operations Research},
  month = {dec},
  number = {3},
  pages = {361--379},
  title = {{Markov Decision Processes with Average-Value-at-Risk criteria}},
  url = {https://doi.org/10.1007/s00186-011-0367-0},
  volume = {74},
  year = {2011},
}

@article{Defourny2008,
  abstract = {This paper considers sequential decision making problems under
              uncertainty,$\backslash$nthe tradeoff between the expected return
              and the risk of high loss,$\backslash$nand methods that use dynamic
              programming to find optimal policies.$\backslash$nIt is argued that
              using Bellman Principle determines how risk considerations$
              \backslash$non the return can be incorporated. The discussion
              centers around$\backslash$nreturns generated by Markov Decision
              Processes and conclusions concern$\backslash$na large class of
              methods in Reinforcement Learning.},
  author = {Defourny, Boris and Ernst, Damien and Wehenkel, Louis},
  file = {:home/soheil/Dropbox/unh/re{\_}learning/documents/nips08{\_}riskaware{
          \_}dp.pdf:pdf},
  journal = {NIPS-08 Workshop on Model Uncertainty and Risk in Reinforcement
             Learning},
  pages = {1--8},
  title = {{Risk-Aware Decision Making and Dynamic Programming}},
  url = {https://orbi.uliege.be/bitstream/2268/15469/1/nips08{\_}riskaware{\_}
         dp.pdf},
  year = {2008},
}

@article{Eldar2008,
  title = {A Minimax {{Chebyshev}} Estimator for Bounded Error Estimation},
  author = {Eldar, Yonina C. and Beck, Amir and Teboulle, Marc},
  year = {2008},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {56},
  number = {4},
  pages = {1388--1397},
  issn = {1053-587X},
  doi = {10.1109/TSP.2007.908945},
  keywords = {_tablet},
  file = {/home/marek/Zotero/storage/9X3XLZ6V/Eldar et al_2008_A minimax
          Chebyshev estimator for.pdf},
}

@article{Wu2013,
  title = {A New Approximate Algorithm for the {{Chebyshev}} Center},
  author = {Wu, Duzhi and Zhou, Jie and Hu, Aiping},
  year = {2013},
  month = aug,
  journal = {Automatica},
  volume = {49},
  number = {8},
  pages = {2483--2488},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2013.04.029},
  abstract = {The parameter or state estimation with bounded noises is getting
              increasingly important in many applications of practical systems
              with some uncertainties. The problem to estimate a deterministic
              parameter or state which is known to lie in an intersection of some
              ellipsoids can be formulated to find the Chebyshev center of the
              intersection set in the case of l2 norm of the estimation error. In
              this paper, an appropriate positive semidefinite relaxation of
              non-convex optimization problem is derived, and then a new
              algorithm for robust minimax estimation is provided. Some examples
              are given to compare the approximate estimate with the existing
              relaxed Chebyshev center.},
  langid = {english},
  keywords = {_tablet,Bounded noise,Chebyshev center,Ellipsoidal estimation,
              Estimation fusion,Positive semidefinite relaxation},
  file = {/home/marek/Zotero/storage/4DG4BVMQ/Wu et al_2013_A new approximate
          algorithm for
          the.pdf;/home/marek/Zotero/storage/VA59HS82/S0005109813002410.html},
}

@misc{chang2021mitigating,
  title = {Mitigating Covariate Shift in Imitation Learning via Offline Data
           Without Great Coverage},
  author = {Jonathan D. Chang and Masatoshi Uehara and Dhruv Sreenivas and Rahul
            Kidambi and Wen Sun},
  year = {2021},
  eprint = {2106.03207},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
}

@inproceedings{Brown2018b,
  title = {Machine Teaching for Inverse Reinforcement Learning: Algorithms and
           Applications},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Brown, Daniel S. and Niekum, Scott},
  year = {2018},
  eprint = {1805.07687},
}

@inproceedings{Brown2018a,
  title = {Risk-Aware Active Inverse Reinforcement Learning},
  booktitle = {Conference on {{Robot Learning}} ({{CoRL}})},
  author = {Brown, Daniel S. and Cui, Yuchen and Niekum, Scott},
  year = {2018},
  eprint = {1901.02161},
}

@article{IRLSurvey2020,
  title = {"A Survey of Inverse Reinforcement Learning: Challenges, Methods and
           Progress"},
  author = {Arora Saurabh, Doshi Prashant},
    journal = "arxiv",
  year = {2020},
}

@article{Ho2016,
  title = {Generative Adversarial Imitation Learning},
  author = {Ho, Jonathan and Ermon, Stefano},
  journal = "NIPS",
  year = {2016},
}

@article{ross_dagger,
    author = {Stéphane Ross,  Geoffrey J. Gordon,  J. Andrew Bagnell},
    title = {"A Reduction of Imitation Learning and Structured Prediction
to No-Regret Online Learning"},
    journal = "AISTATS",
    year = {2011}, 
}
